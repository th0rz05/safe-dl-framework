attack_overrides:
  backdoor:
    learned_trigger:
      epochs: 30
      label_mode: corrupted
      lambda_mask: 0.001
      lambda_tv: 0.01
      learning_rate: 0.1
      patch_size_ratio: 0.05
      poison_fraction: 0.05
      target_class: 1
    static_patch:
      blend_alpha: 1.0
      label_mode: corrupted
      patch_position: bottom_right
      patch_size_ratio: 0.15
      patch_type: white_square
      poison_fraction: 0.05
      target_class: 0
  data_poisoning:
    clean_label:
      epsilon: 0.1
      fraction_poison: 0.05
      max_iterations: 100
      perturbation_method: overlay
      source_selection: random
      target_class: 2
dataset:
  name: cifar10
  type: builtin
defense_config:
  backdoor:
    learned_trigger:
      activation_clustering:
        num_clusters: 2
      anomaly_detection:
        type: autoencoder
      defenses:
      - activation_clustering
      - spectral_signatures
      - anomaly_detection
      - pruning
      - fine_pruning
      - model_inspection
      fine_pruning:
        pruning_ratio: 0.2
      model_inspection:
        layers:
        - fc1
        - fc2
      pruning:
        pruning_ratio: 0.2
        scope: all_layers
      spectral_signatures:
        threshold: 0.9
  data_poisoning: {}
  evasion_attacks: {}
model:
  input_shape:
  - 3
  - 32
  - 32
  name: cnn
  num_classes: 10
  params:
    conv_filters: 32
    hidden_size: 128
  type: builtin
name: test
risk_analysis:
  recommendations:
    clean_label:
    - provenance_tracking
    - influence_functions
    cw:
    - adversarial_training
    - randomized_smoothing
    - certified_defense
    deepfool:
    - adversarial_training
    - randomized_smoothing
    - certified_defense
    fgsm:
    - adversarial_training
    - randomized_smoothing
    - certified_defense
    label_flipping:
    - data_cleaning
    - per_class_monitoring
    learned_trigger:
    - activation_clustering
    - spectral_signatures
    - fine_pruning
    - model_inspection
    nes:
    - gradient_masking
    - jpeg_preprocessing
    pgd:
    - adversarial_training
    - randomized_smoothing
    - certified_defense
    spsa:
    - gradient_masking
    - jpeg_preprocessing
    transfer:
    - adversarial_training
    - randomized_smoothing
    - certified_defense
  summary:
    clean_label:
      probability: 0.9
      risk_score: 0.244
      severity: 0.159
      visibility: 0.3
    cw:
      probability: 0.9
      risk_score: 1.53
      severity: 1.0
      visibility: 0.3
    deepfool:
      probability: 1.0
      risk_score: 1.9
      severity: 1.0
      visibility: 0.1
    fgsm:
      probability: 1.0
      risk_score: 1.7
      severity: 1.0
      visibility: 0.3
    label_flipping:
      probability: 1.0
      risk_score: 0.547
      severity: 0.422
      visibility: 0.705
    learned_trigger:
      probability: 0.9
      risk_score: 1.62
      severity: 1.0
      visibility: 0.2
    nes:
      probability: 0.8
      risk_score: 1.44
      severity: 1.0
      visibility: 0.2
    pgd:
      probability: 1.0
      risk_score: 1.7
      severity: 1.0
      visibility: 0.3
    spsa:
      probability: 0.8
      risk_score: 1.44
      severity: 1.0
      visibility: 0.2
    static_patch:
      probability: 1.0
      risk_score: 1.4
      severity: 1.0
      visibility: 0.6
    transfer:
      probability: 0.85
      risk_score: 1.53
      severity: 1.0
      visibility: 0.2
threat_model:
  attack_goal: targeted
  data_sensitivity: high
  deployment_scenario: cloud
  interface_exposed: api
  model_access: white-box
  model_type: cnn
  threat_categories:
  - data_poisoning
  - backdoor_attacks
  - evasion_attacks
  - model_stealing
  - membership_inference
  - model_inversion
  training_data_source: internal_clean
