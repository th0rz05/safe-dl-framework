# Module 3 — Risk Analysis

## Table of Contents

- [1. Introduction](#1-introduction)
- [2. Objectives](#2-objectives)
- [3. Inputs](#3-inputs)
- [4. Outputs](#4-outputs)
  * [4.1 risk_analysis.json](#41-risk-analysisjson)
  * [4.2 risk_report.md](#42-risk-reportmd)
  * [4.3 profile.yaml updates](#43-profileyaml-updates)
- [5. Risk Modeling](#5-risk-modeling)
  * [5.1 Severity](#51-severity)
  * [5.2 Probability](#52-probability)
  * [5.3 Visibility](#53-visibility)
  * [5.4 Score Normalization](#54-score-normalization)
- [6. Implementation Overview](#6-implementation-overview)
  * [6.1 Main Components](#61-main-components)
  * [6.2 Directory Structure](#62-directory-structure)
  * [6.3 Execution Flow](#63-execution-flow)
- [7. Example Output](#7-example-output)
  * [7.1 risk_analysis.json](#71-risk-analysisjson)
  * [7.2 risk_report.md](#72-risk-reportmd)
- [8. Integration with Module 4](#8-integration-with-module-4)
  * [8.1 Fields Added to profile.yaml](#81-fields-added-to-profileyaml)
- [9. Limitations and Future Work](#9-limitations-and-future-work)
  * [9.1 Fixed Risk Weights](#91-fixed-risk-weights)
  * [9.2 Attack-Type Coverage](#92-attack-type-coverage)
  * [9.3 Risk History Tracking](#93-risk-history-tracking)
  * [9.4 Defense Effectiveness Feedback](#94-defense-effectiveness-feedback)
  * [9.5 Visualization and UI](#95-visualization-and-ui)


## 1. Introduction

Module 3 of the Safe-DL framework is responsible for assessing the security impact of adversarial attacks simulated in Module 2. While Module 2 focuses on executing concrete attack strategies against a given model and dataset, Module 3 transitions from raw performance metrics to structured risk interpretation.

This module consolidates the attack results — such as accuracy degradation, attack success rate (ASR), and perturbation visibility — and transforms them into high-level risk indicators. These indicators include:

- **Severity** — how damaging the attack is to model performance;
- **Probability** — the likelihood of the attack succeeding given the threat model;
- **Visibility** — how easily the attack can be detected.

By modeling these dimensions, Module 3 assigns a **final risk score** to each attack. The results are used to:

- Rank and compare the most dangerous attacks;
- Provide tailored defense recommendations;
- Automatically update the profile file for downstream defense selection in Module 4.

Ultimately, this module bridges the gap between empirical attack data and informed defense planning, bringing the Safe-DL framework closer to practical deployment in adversarial settings.



## 2. Objectives

The main goal of Module 3 is to translate raw attack performance metrics into structured and interpretable risk assessments. These assessments support decision-making in the defense phase and help prioritize which threats need immediate mitigation.

Specifically, this module aims to:

- **Analyze attack impact** by comparing clean model performance to its performance under attack conditions.
- **Compute standardized risk metrics** for each attack, including severity, probability, and visibility.
- **Generate a final risk score** to allow cross-attack comparison and prioritization.
- **Produce a structured JSON file (`risk_analysis.json`)** containing risk data for each executed attack.
- **Create a Markdown report (`risk_report.md`)** summarizing the risk landscape with tables, rankings, risk matrix, and recommendations.
- **Update the profile (`profile.yaml`)** with:
  - A compact summary of risk metrics (`risk_analysis.summary`);
  - A set of recommended defenses for each attack (`risk_analysis.recommendations`).

These outputs are then used in Module 4 to assist the user in selecting the most appropriate defense strategies for their scenario.



## 3. Inputs

Module 3 depends on structured outputs from the previous stages of the Safe-DL framework. The following inputs are required:

- **`profile.yaml`**:  
  The threat profile selected by the user in Module 1. This file specifies which attacks were executed, the dataset and model configuration, and threat model parameters. It also serves as the storage location for the risk analysis summary and recommendations generated by this module.

- **Attack metric files from Module 2**:  
  Each attack implemented in Module 2 outputs a corresponding `*_metrics.json` file containing evaluation results. These files include fields such as:
  - Clean vs adversarial accuracy;
  - Per-class accuracy;
  - Attack-specific parameters (e.g., flip rate, ASR, perturbation norm);
  - Other metadata used to compute risk indicators.

  Example files include:
  - `results/data_poisoning/label_flipping/label_flipping_metrics.json`
  - `results/backdoor/static_patch/static_patch_metrics.json`
  - `results/evasion/fgsm/fgsm_metrics.json`

- **`baseline_accuracy.json`**:  
  A reference file containing the clean (unattacked) performance of the model. It is used as a baseline for measuring accuracy degradation caused by each attack.


## 4. Outputs

Module 3 generates several outputs that summarize the risk posed by each adversarial attack and facilitate the next stage of defense planning.

### 4.1 risk_analysis.json

A structured JSON file where each key corresponds to an executed attack, and each value contains its computed risk metrics:

- `severity`: how much the attack harms model performance;
- `probability`: estimated likelihood of attack success;
- `visibility`: degree to which the attack is perceptible;
- `risk_score`: final value computed from the three metrics;
- Other contextual fields, such as flip rate or ASR.

This file is saved in:  
`results/risk_analysis.json`

---

### 4.2 risk_report.md

A human-readable Markdown report that includes:

- A summary table comparing all attacks by severity, probability, visibility, and risk score;
- A qualitative risk matrix based on severity and probability buckets;
- A ranked list of attacks from most to least risky;
- Direct links to the individual attack reports from Module 2;
- Tailored defense recommendations for each attack.

This file is saved in:  
`results/risk_report.md`

---

### 4.3 profile.yaml updates

The threat profile used in the analysis is updated with a new section:

```yaml
risk_analysis:
  summary:
    attack_name:
      severity: ...
      probability: ...
      visibility: ...
      risk_score: ...
  recommendations:
    attack_name:
      - recommendation line 1
      - recommendation line 2

```

These updates allow Module 4 to access risk metrics and defense suggestions without reprocessing results.

## 5. Risk Modeling

In Safe-DL, risk is modeled as a combination of three core factors:

- **Severity**: how strongly the attack degrades model performance;
- **Probability**: how likely the attack is to succeed in the defined threat model;
- **Visibility**: how perceptible or stealthy the attack is to a human observer or detection mechanism.

These factors are combined to compute a final `risk_score` for each attack:


```
risk_score = severity × probability × (1 + (1 - visibility))
```

This formulation ensures that:
- Highly severe and likely attacks receive a higher score;
- Low visibility (i.e., stealthy) attacks increase risk due to being harder to detect.

---

### 5.1 Severity

Severity is primarily measured by the **drop in accuracy** compared to the clean baseline (from `baseline_accuracy.json`). It is normalized to a 0–1 scale using predefined thresholds. For some attacks, like backdoors, severity may also include **attack success rate (ASR)**.

Example:
```python
severity = min((accuracy_clean - accuracy_attack) / 0.3, 1.0)
```

----------

### 5.2 Probability

Probability is estimated based on:

-   The threat model defined in `profile.yaml` (e.g., white-box vs black-box);
    
-   The nature of the attack (e.g., label flipping is easier than a learned trigger);
    
-   Default values are currently assigned (e.g., `1.0` for white-box, `0.8` for black-box), but can be made adaptive in future versions.
    

----------

### 5.3 Visibility

Visibility estimates how perceptible the attack is. It varies by attack type:

-   For evasion attacks: based on average perturbation norm (if available);
    
-   For poisoning: based on number or fraction of flipped/poisoned samples;
    
-   For backdoors: based on trigger blending factor (`blend_alpha`) or patch size.
    

Lower visibility implies stealthier attacks, which increases their risk.

----------

### 5.4 Score Normalization

All computed metrics are clamped to the `[0, 1]` range and rounded to 3 decimal places for consistency and interpretability.


## 6. Implementation Overview

Module 3 is implemented as a set of modular Python scripts, each with a clearly defined responsibility. The implementation reads results from Module 2, computes risk metrics, generates reports, and updates the profile file for future use.

---

### 6.1 Main Components

- **`run_module3.py`**  
  Entry point for the module. It loads the selected profile, identifies which attacks were executed, loads the corresponding `*_metrics.json` files, and calls the appropriate analysis function from `risk_utils.py`. The results are saved to `risk_analysis.json`.

- **`risk_utils.py`**  
  Contains per-attack risk analysis functions. Each function is tailored to a specific attack type and calculates:
  - Severity (e.g., based on accuracy drop or ASR);
  - Probability (based on attacker assumptions and ease of execution);
  - Visibility (based on perturbation size, flip rate, trigger type, etc.).

  Example functions:
  - `analyze_fgsm(data, baseline)`
  - `analyze_label_flipping(data, baseline)`
  - `analyze_static_patch(data)`

- **`generate_risk_report.py`**  
  Reads the `risk_analysis.json` and `profile.yaml`, and generates a comprehensive Markdown report:
  - Includes summary table, qualitative matrix, and ranking;
  - Appends links to each detailed attack report;
  - Generates per-attack defense recommendations based on the metrics;
  - Updates the `profile.yaml` with a summary of the metrics and recommendations under `risk_analysis`.

---

### 6.2 Directory Structure

Module 3 is implemented under its own folder:

```
src 
  ├── module3_risk_analysis
    ├── run_module3.py
    ├── risk_utils.py
    ├── generate_risk_report.py
    └──results
      ├── risk_analysis.json 
      └── risk_report.md

```

---

### 6.3 Execution Flow

1. The user selects a threat profile using `run_module3.py`.
2. The script identifies which attacks were performed and analyzes their results.
3. A risk dictionary is built and saved to `risk_analysis.json`.
4. `generate_risk_report.py` generates:
   - A Markdown report summarizing the risks;
   - A `risk_analysis` section in `profile.yaml`, with:
     - `summary` per attack;
     - `recommendations` for defenses.

This implementation ensures that Module 3 is self-contained, modular, and extensible to future attacks and metrics.

## 7. Example Output

This section illustrates typical outputs produced by Module 3 after analyzing the attacks executed in Module 2.

---

### 7.1 risk_analysis.json

This JSON file contains structured risk metrics for each attack. Below is a simplified example of the output for multiple attack types:

```json
{
  "label_flipping": {
    "type": "data_poisoning",
    "accuracy_clean": 0.6754,
    "accuracy_attack": 0.5488,
    "severity": 0.422,
    "probability": 1.0,
    "visibility": 0.705,
    "risk_score": 0.547,
    "flip_rate": 0.1,
    "num_flipped": 4049
  },
  "static_patch": {
    "type": "backdoor",
    "accuracy_clean": 0.6058,
    "asr": 0.7799,
    "severity": 1.0,
    "probability": 1.0,
    "visibility": 0.6,
    "risk_score": 1.4,
    "patch_size_ratio": 0.15,
    "poison_fraction": 0.05,
    "blend_alpha": 0.25
  },
  "pgd": {
    "type": "evasion",
    "accuracy_clean": 0.6754,
    "accuracy_attack": 0.0,
    "severity": 1.0,
    "probability": 1.0,
    "visibility": 0.3,
    "risk_score": 1.7,
    "epsilon": 0.03,
    "pgd_steps": 10
  }
}

```

Each entry contains both the core risk metrics and additional context-specific fields extracted from the original metric files.

----------

### 7.2 risk_report.md

This Markdown report includes:

-   A summary table comparing all attacks by key metrics.
    
-   A qualitative risk matrix mapping severity × probability.
    
-   A ranked list of the riskiest attacks.
    
-   Direct links to the detailed attack reports (`*_report.md` from Module 2).
    
-   Tailored recommendations for each attack.
    

 **Excerpt from the report:**

```
## Risk Ranking

1. **pgd** — risk score: 1.70 → [Report](../module2_attack_simulation/results/evasion/pgd/pgd_report.md)  
2. **static_patch** — risk score: 1.40 → [Report](../module2_attack_simulation/results/backdoor/static_patch/static_patch_report.md)  
3. **label_flipping** — risk score: 0.55 → [Report](../module2_attack_simulation/results/data_poisoning/label_flipping/label_flipping_report.md)

## Recommendations

- **pgd**: Very high-risk evasion attack. Recommend adversarial training, randomized smoothing, or certified defenses.
- **static_patch**: Fully blended backdoor trigger. Use activation clustering and spectral signature defenses.
```

This output allows researchers and developers to quickly interpret which attacks are most dangerous, why, and how to defend against them.


## 8. Integration with Module 4

Module 3 directly informs Module 4, which is responsible for selecting and applying defense strategies to mitigate the most critical threats.

The integration is achieved by embedding the risk analysis results and recommendations into the `profile.yaml` used throughout the framework. This enables Module 4 to:

- **Access structured risk scores** for each attack without reprocessing metrics.
- **Filter or prioritize defenses** based on the most severe or stealthy attacks.
- **Provide automated recommendations** to the user based on the risk profile.
- **Allow manual override**, giving the user full control over which defenses to apply regardless of suggestions.

---

### 8.1 Fields Added to profile.yaml

After Module 3 execution, the profile file is updated with:

```yaml
risk_analysis:
  summary:
    label_flipping:
      severity: 0.422
      probability: 1.0
      visibility: 0.705
      risk_score: 0.547
    pgd:
      severity: 1.0
      probability: 1.0
      visibility: 0.3
      risk_score: 1.7
  recommendations:
    label_flipping:
      - Flip rate above 5%. Recommend data cleaning and per-class accuracy monitoring.
    pgd:
      - Very high-risk evasion attack. Recommend adversarial training, randomized smoothing, or certified defenses.
```

----------


## 9. Limitations and Future Work

While Module 3 already provides a robust foundation for risk assessment in adversarial settings, there are several areas that could be improved or extended in future iterations.


### 9.1 Fixed Risk Weights

The current implementation uses fixed weights and static rules to compute the final `risk_score`. Although this works well for general scenarios, it may not capture context-specific priorities. In future versions, the framework could allow:

- **Custom weight configurations** defined in the `profile.yaml`;
- Dynamic scoring models that adapt based on deployment scenario or threat model.

---

### 9.2 Attack-Type Coverage

At this stage, Module 3 only analyzes the following types of attacks:

- Data poisoning (e.g., label flipping, clean label);
- Backdoor attacks (e.g., static patch, learned trigger);
- Evasion attacks (e.g., FGSM, PGD, C&W, DeepFool, black-box variants).

Attacks such as **model stealing**, **membership inference**, and **model inversion** are defined in the threat model but not yet implemented in the risk analysis phase. Support for these will be added as they are integrated into Module 2.

---

### 9.3 Risk History Tracking

Currently, only the latest risk analysis is saved to the profile. It could be beneficial to support:

- **Versioned logging of risk assessments**;
- Time-stamped report archives;
- Comparison between different attack rounds or profile variations.

---

### 9.4 Defense Effectiveness Feedback

The current risk analysis is static. In the future, the system could integrate feedback from Module 4 to:

- Measure how effective the chosen defenses were in reducing risk;
- Re-score attacks post-defense;
- Recommend defense upgrades based on residual vulnerabilities.

---

### 9.5 Visualization and UI

The Markdown report is informative but static. A future improvement would be to offer:

- A web-based interface or dashboard;
- Interactive risk visualizations (e.g., radar charts, dynamic matrices);
- Export options to PDF or HTML.

---

These improvements would further enhance the Safe-DL framework’s ability to support real-world deployment scenarios and continuous risk monitoring.
