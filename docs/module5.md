
# Module 5 — Defense Evaluation

This module assesses the effectiveness of all defenses applied in the Safe-DL framework.  
After simulating attacks (Module 2) and applying countermeasures (Module 4), Module 5 computes a set of scores to quantify each defense's impact, trade-offs, and overall benefit.

It reads the raw metrics collected from previous modules and produces a structured evaluation report, both in `.json` and `.md` format, helping users make informed decisions about which defenses are most suitable for their threat model and deployment context.


## Table of Contents

- [1. Module Overview](#1-module-overview)
- [2. Inputs and Outputs](#2-inputs-and-outputs)
  * [2.1 Inputs](#21-inputs)
  * [2.2 Outputs](#22-outputs)
- [3. Scoring Methodology](#3-scoring-methodology)
  * [3.1 Mitigation Score](#31-mitigation-score)
  * [3.2 Clean Accuracy Drop (CAD) Score](#32-clean-accuracy-drop--cad--score)
  * [3.3 Cost Score](#33-cost-score)
  * [3.4 Final Score](#34-final-score)
- [4. Implementation Details](#4-implementation-details)
  * [4.1 Input Files](#41-input-files)
  * [4.2 Output Files](#42-output-files)
  * [4.3 Execution Flow](#43-execution-flow)
  * [4.4 Modular Design](#44-modular-design)
- [5. Results Summary](#5-results-summary)
  * [5.1 Summary Table](#51-summary-table)
  * [5.2 Notable Observations](#52-notable-observations)
  * [5.3 Report Generation](#53-report-generation)
- [6. Limitations and Future Work](#6-limitations-and-future-work)
  * [6.1 Current Limitations](#61-current-limitations)
  * [6.2 Planned Enhancements](#62-planned-enhancements)
- [7. Conclusion](#7-conclusion)


## 1. Module Overview

Module 5 is responsible for evaluating the defenses applied to deep learning models in response to previously simulated adversarial threats.  
It aggregates results from:

-   The **original clean model** (baseline performance),
    
-   The **attacked model** (performance after the attack), and
    
-   The **defended model** (performance after the defense).
    

The goal is to quantify the **effectiveness**, **efficiency**, and **impact** of each defense using standardized scores.

This module supports evaluation across all attack categories considered in Module 2:

-   **Data Poisoning Attacks** (e.g., Label Flipping)
    
-   **Backdoor Attacks** (e.g., Static and Learned Triggers)
    
-   **Evasion Attacks** (e.g., PGD, SPSA)
    

Each defense is scored individually for each attack, and all results are saved to:

-   A machine-readable file: `defense_evaluation.json`
    
-   A human-readable report: `defense_evaluation_report.md`
   

## 2. Inputs and Outputs

This module relies on inputs generated by previous modules, particularly Modules 2 and 4. It processes these files to compute evaluation scores for each defense.


### 2.1 Inputs

The module takes the following as inputs:

* **Attack Results (from Module 2):** JSON files containing metrics of the attacks, stored under `../module2_attack_simulation/results/<attack_category>/<attack_name>/<attack_name>_metrics.json`. These include metrics like `accuracy_adversarial_testset` for evasion or `attack_success_rate` for backdoor attacks, depending on the attack type.
* **Defense Application Results (from Module 4):** JSON files detailing the performance of the model *after* defenses have been applied, located at `../module4_defense_application/results/<attack_category>/<attack_name>/<defense_name>_results.json`. These files contain `accuracy_clean` and `accuracy_adversarial` (or `asr_after_defense` for backdoor attacks).
* **Baseline Accuracy:** A JSON file (`../module2_attack_simulation/results/baseline_accuracy.json`) providing the `overall_accuracy` of the model on clean data *before* any attacks or defenses. This baseline is critical for calculating the true impact of both attacks and defenses.
* **Threat Profile (from Module 1):** The `profile.yaml` file, which specifies the attack scenarios and defense configurations, guiding which results to load and evaluate.
    

### 2.2 Outputs

-   `defense_evaluation.json`  
    A structured JSON file containing defense scores for each `(attack, defense)` pair.
    
-   `defense_evaluation_report.md`  
    A Markdown report summarizing the results for humans, with tables, profile info, and contextual notes.
    
## 3. Scoring Methodology

This module evaluates each defense based on a combination of performance recovery, trade-offs on clean data, computational cost, and coverage. The following metrics are computed:

### 3.1 Mitigation Score

Measures how effectively the defense recovers accuracy lost due to the attack:

**Mitigation Score** = (Accuracy_defense - Accuracy_attack) / (Accuracy_baseline - Accuracy_attack)

If the attack caused no degradation, the score is 0.

### 3.2 Clean Accuracy Drop (CAD) Score

Evaluates the cost of the defense on clean data. A higher score indicates less degradation:

**CAD Score** = max(0.0, 1.0 - (Accuracy_baseline - Accuracy_clean_after_defense) / Max_Drop)

A max drop of 10% is used as the acceptable threshold by default.

### 3.3 Cost Score

Represents the computational or implementation cost of the defense. It is based on a predefined internal mapping with values between `0.0` (cheap) and `1.0` (expensive). Example mapping:

| Defense                | Cost |
|------------------------|------|
| data_cleaning          | 0.2  |
| adversarial_training   | 0.8  |
| certified_defense      | 0.9  |
| jpeg_preprocessing     | 0.1  |

### 3.4 Final Score

Aggregates all metrics into a single score, rewarding effectiveness and efficiency while penalizing expensive defenses:

**Final Score** = ((Mitigation × CAD) × (0.5 + 0.5 × PCR) × (0.5 + 0.5 × Coverage)) / (1 + Cost)

Where:
- **PCR**: Per-Class Recovery (default: 1.0 if not available)
- **Coverage**: Class-wise coverage (default: 1.0)
- **Cost**: Cost Score


## 4. Implementation Details

This module is implemented in the file `run_module5.py` and leverages all previously generated attack and defense results from Modules 2 and 4. It also uses scoring logic from `defense_score_utils.py` and evaluation logic per defense in `defense_utils.py`.

### 4.1 Input Files

The following inputs are required to run the evaluation:

- `baseline_accuracy.json`  
  Contains the baseline accuracy of the model before any attacks or defenses.

- `<attack_type>_metrics.json` (from Module 2)  
  Contains clean and adversarial accuracy results for each attack.

- `<defense_name>_results.json` (from Module 4)  
  Contains results of applying a specific defense to a specific attack.

- `<profile>.yaml`  
  Specifies the dataset, model, selected attacks, and defense configurations.

### 4.2 Output Files

The module produces two main outputs:

- `defense_evaluation.json`  
  A structured summary of all computed scores per defense and attack, including mitigation, clean accuracy drop (CAD), cost, and final score.

- `defense_evaluation_report.md`  
  A Markdown report summarizing the defense effectiveness across all attacks and categories, ready to be included in the thesis.


### 4.3 Execution Flow

The `run_module5.py` script orchestrates the evaluation process:

1.  **Profile Selection:** The user selects a threat profile (`.yaml` file) that defines the attacks and their corresponding defenses to be evaluated.
2.  **Baseline Loading:** The baseline accuracy of the clean model is loaded.
3.  **Iterate through Attacks:** For each attack category and specific attack defined in the profile:
    * The relevant attack metrics are loaded from Module 2's results.
    * Based on the `defense_config` in the threat profile (which, in turn, is informed by `defense_tags.py`), the script identifies which defenses were applied and should be evaluated for the current attack.
    * For each applied defense, its results are loaded from Module 4.
    * The appropriate generic evaluation function (`evaluate_backdoor_defense`, `evaluate_data_poisoning_defense`, or `evaluate_evasion_defense`) is called to compute the mitigation, CAD, cost, and final scores.
4.  **Results Aggregation & Saving:** All computed scores are aggregated into a single JSON file (`defense_evaluation.json`).
5.  **Report Generation:** A human-readable Markdown report (`defense_evaluation_report.md`) is generated summarizing the evaluation.


### 4.4 Modular Design

The module is designed for modularity, allowing for easy extension to new attacks and defenses.
Evaluation logic for different defense categories is encapsulated in dedicated functions within `defense_score_utils.py`:

* `evaluate_backdoor_defense`: Handles scoring for defenses against backdoor attacks.
* `evaluate_data_poisoning_defense`: Manages scoring for defenses against data poisoning attacks.
* `evaluate_evasion_defense`: Provides scoring for defenses against evasion attacks.

This approach ensures that the scoring methodology remains consistent across all defenses within a category, simplifying maintenance and future additions.

## 5. Results Summary

This section presents the results obtained from the evaluation of all defenses applied in Module 4. Each defense was tested against the corresponding attack(s) defined in the selected threat profile.

The key metrics reported are:

- **Mitigation Score**: Measures the improvement in accuracy on adversarial examples after applying the defense.
- **CAD (Clean Accuracy Drop)**: Quantifies the drop in model accuracy on clean (non-adversarial) data.
- **Cost Score**: Reflects the estimated computational or architectural cost of the defense.
- **Final Score**: Aggregated score combining mitigation, CAD, and cost, providing a global perspective of the defense effectiveness.

### 5.1 Summary Table

The following table presents a condensed view of the evaluation results. Higher final scores indicate more effective and efficient defenses.

<!-- This table is programmatically generated using generate_defense_evaluation_report.py -->

| Attack | Defense | Mitigation | CAD | Cost | Final Score |
|--------|---------|------------|-----|------|--------------|
| label_flipping | data_cleaning | 0.211 | -0.089 | 0.200 | 0.108 |
| label_flipping | per_class_monitoring | 0.163 | -0.021 | 0.300 | 0.147 |
| pgd | adversarial_training | 0.194 | -0.231 | 0.300 | 0.045 |
| spsa | jpeg_preprocessing | 0.167 | -0.065 | 0.100 | 0.134 |


_Note: The above table is illustrative. The actual table is dynamically generated based on the current results stored in `defense_evaluation.json`._

### 5.2 Notable Observations

-   Some defenses significantly improve robustness against certain attack types (e.g., `data_cleaning` for label flipping), while others offer moderate benefits with minimal accuracy degradation.
    
-   Defenses with high computational cost (e.g., `adversarial_training`) may lower the overall final score despite offering strong mitigation.
    
-   Lightweight preprocessing methods (e.g., `jpeg_preprocessing`) tend to offer a good balance between mitigation and cost, especially against black-box attacks.
    

### 5.3 Report Generation

The comprehensive defense evaluation report, `defense_evaluation_report.md`, is automatically generated by the `generate_defense_evaluation_report.py` script. This script processes the raw scores from `defense_evaluation.json` and presents them in a human-readable Markdown format.

The report includes:

-   **High-level overview**: Explaining the purpose and methodology of Module 5.
-   **Summary Table**: A dynamically generated table (as shown in Section 5.1) that provides a quick comparative view of all evaluated defenses against different attacks. This table includes the Mitigation Score, CAD Score, Cost Score, and the Final Score for each `(attack, defense)` pair.
-   **Contextual Notes**: Explanations for each scoring metric to help interpret the results correctly.
-   **Profile Information**: Details about the selected threat profile, including the dataset and model used, ensuring traceability and reproducibility.

This Markdown report is designed to be easily integrated into broader documentation, such as a thesis or project report, providing a clear and concise summary of the defense effectiveness. It facilitates decision-making by allowing users to quickly identify the most effective and efficient defenses for their specific adversarial scenarios.

## 6. Limitations and Future Work

This section outlines the current limitations of Module 5 and discusses planned enhancements to further improve its capabilities.

### 6.1 Current Limitations

While Module 5 provides a robust evaluation of defense mechanisms, there are some aspects that represent current limitations:

* **Fixed Cost Modeling**: The `Cost Score` is currently based on a predefined, static mapping (`COST_MAP` in `defense_utils.py`). This offers a general estimate but does not dynamically account for variations in computational resources, model size, dataset size, or specific hardware configurations, which can significantly influence the actual cost in real-world scenarios.
* **Default PCR and Coverage Scores**: For certain defense types or evaluation scenarios, the `Per-Class Recovery (PCR)` and `Coverage` metrics within the `Final Score` formula currently default to `1.0` if specific data for these metrics is not available from the defense application. This assumes full effectiveness or coverage, which may not always be accurate and can lead to an overestimation of the defense's true performance in such cases.
* **Static `Max_Drop` for CAD**: The `max_allowed_drop` for the `Clean Accuracy Drop (CAD)` score is set to a fixed threshold (e.g., 10%). While a reasonable default, an optimal threshold can vary depending on the application's tolerance for clean accuracy degradation. This fixed value might not perfectly reflect the acceptable trade-offs for all deployment contexts.
* **No Direct Visualization**: The output is primarily a Markdown table and a JSON file. While informative, it lacks interactive visualizations (e.g., bar charts, radar charts) that could offer more intuitive insights into defense trade-offs and comparative performance.

### 6.2 Planned Enhancements

To address the current limitations and further enhance the utility of Module 5, the following improvements are planned:

* **Dynamic Cost Modeling**: Future versions will explore more sophisticated cost models that can dynamically estimate the defense's impact based on factors like model size, dataset dimensions, training/inference time, and computational resources required (e.g., GPU memory, CPU usage). This will allow for a more accurate and context-aware `Cost Score`.
* **Expanded Metric Integration**: Integrate more granular metrics, especially for `Per-Class Recovery (PCR)` and `Coverage`, as more sophisticated attack and defense modules are developed. This will ensure that the `Final Score` fully incorporates these aspects where relevant, providing a more precise evaluation.
* **Configurable CAD Threshold**: Allow users to define the `max_allowed_drop` for the `Clean Accuracy Drop (CAD)` score within the threat profile (`profile.yaml`). This will enable customization of the acceptable trade-off between robustness and clean accuracy based on the specific application's requirements.
* **Integrated Visualization**: Develop capabilities for generating interactive plots and charts (e.g., comparative bar charts, radar plots for multi-metric comparison) directly from the evaluation results. This will provide more intuitive insights and facilitate easier analysis of defense performance.
* **Feedback Loop with Risk Analysis**: Enhance the integration with Module 3 (Risk Analysis) to feed back defense evaluation scores. This would allow for a dynamic re-assessment of residual risks after defenses are applied, providing a more complete picture of the security posture.


## 7. Conclusion

Module 5 serves as the critical evaluation hub of the Safe-DL framework, transforming raw attack and defense metrics into actionable insights. By systematically quantifying defense effectiveness, trade-offs, and costs, this module provides users with a standardized, data-driven approach to understand the true impact of implemented countermeasures.

The `defense_evaluation.json` and `defense_evaluation_report.md` outputs empower users to:

* **Make Informed Decisions**: Compare different defense strategies based on concrete performance metrics, facilitating the selection of the most suitable defenses for specific threat models and deployment contexts.
* **Understand Trade-offs**: Clearly identify the balance between adversarial robustness gains and potential degradations in clean accuracy or increases in operational cost.
* **Drive Iterative Improvements**: The detailed evaluation highlights areas where defenses are most effective and where further research or tuning might be required, fostering a continuous improvement cycle for ML model security.

Ultimately, Module 5 ensures that defense application within the Safe-DL framework is not a blind deployment but a strategic, evidence-based process aimed at building more resilient deep learning systems.
