
# Module 4 — Defense Application

This module is responsible for applying defense mechanisms based on the threat profile previously defined and the attacks simulated in Module 2. Using the risk analysis performed in Module 3, it guides the selection, configuration, and execution of defenses to mitigate the effects of adversarial attacks on deep learning models.

## 1. Introduction

As deep learning models are increasingly exposed to adversarial threats in real-world applications, the ability to assess and mitigate these threats is essential. Module 4 addresses this need by systematically applying defense strategies tailored to the specific attacks identified in previous phases of the Safe-DL framework.

Each defense is applied in a structured pipeline that re-simulates the threat scenario, applies the mitigation, retrains the model, and evaluates post-defense performance. This ensures a consistent and comparative assessment of the effectiveness of each strategy.

The module supports both data-centric and model-centric defenses, and is designed to be extensible to various types of attacks, including poisoning, evasion, and backdoor threats. Configuration and control are managed via YAML profiles, ensuring reproducibility and transparency throughout the process

----------


## 2. Setup Phase

Before any defenses are applied, the user must select a previously generated threat profile and confirm which defense strategies to apply. This setup step ensures that defenses are aligned with the attack vectors detected in Module 2 and the risk assessments generated in Module 3.

### 2.1 Loading the Threat Profile

The user begins by selecting a `.yaml` threat profile generated by Module 1. This profile encapsulates:

-   Dataset and model specifications;
    
-   Attack configurations and metrics (from Module 2);
    
-   Risk levels and suggested mitigations (from Module 3).
    

This file acts as the central control point for guiding the defense logic. Upon loading, the system extracts relevant metadata (such as attack types and targets) to determine which defenses are available and applicable.

### 2.2 Interactive Defense Selection

Once the profile is loaded, the framework presents a list of recommended defenses for each attack type, based on mappings defined in `defense_tags.py`. Users can choose to:

-   Accept the system’s recommendations;
    
-   Customize the selected defenses per attack;
    
-   Configure specific parameters for each method.
    

All choices are stored under the `defense_config` section of the profile file. This design allows for full traceability and reproducibility of the defense setup phase, as the exact configuration used during execution is saved and version-controlled
        

----------

## 3. Implemented Defenses

This section documents the defense mechanisms currently implemented in the Safe-DL framework. Each subsection corresponds to one defense and includes its logic, applicable attack types, configurable parameters, and output structure.

### 3.1 Data Cleaning

**Description**  
Data cleaning aims to remove suspicious or mislabeled training samples that may degrade model performance. It operates under the assumption that poisoned data tends to produce unusually high training loss, making it detectable through statistical analysis.

**Applicable to:**

-   `label_flipping` (Data Poisoning)
    

**Configuration Parameters**  
The user can select the cleaning method and its threshold:

-   `method`:
    
    -   `"loss_filtering"`: retains samples with loss below a scaled mean;
        
    -   `"outlier_detection"`: applies z-score thresholding to loss values.
        
-   `threshold`: a float value controlling filtering strictness.
    

These parameters are configured interactively during the setup phase and stored in the profile under:

```yaml
defense_config:
  data_poisoning:
    label_flipping:
      data_cleaning:
        method: "loss_filtering"
        threshold: 0.9

```

**Pipeline Summary**

1.  The original attack (label flipping) is re-applied to simulate the poisoned dataset.
    
2.  A helper model is trained on the poisoned dataset for loss estimation.
    
3.  Data cleaning is applied to remove high-loss samples.
    
4.  A new model is trained on the cleaned dataset and evaluated.
    

**Outputs**

-   `results/data_poisoning/label_flipping/data_cleaning_results.json`  
    Contains metrics such as post-defense accuracy, removed indices, and parameters used.
    
-   `results/data_poisoning/label_flipping/data_cleaning_report.md`  
    Markdown report with performance summary and visual examples.
    
-   `results/data_poisoning/label_flipping/cleaned_examples/`  
    Contains up to 5 images of removed samples with their labels.
    

----------
   

### 3.2 Per-Class Monitoring

**Description**  
Per-class monitoring is a lightweight diagnostic defense that analyzes validation accuracy across all classes after training on potentially poisoned data. The goal is to identify target classes that have experienced an unusual drop in performance, which may indicate class-specific attacks such as label flipping.

**Applicable to:**

-   `label_flipping` (Data Poisoning)
    

**Configuration Parameters**  
This method requires a single parameter:

-   `std_threshold`:  
    The number of standard deviations below the mean accuracy used as a threshold to flag abnormal classes. Defaults to `2.0`.
    

YAML configuration example:

```yaml
defense_config:
  data_poisoning:
    label_flipping:
      per_class_monitoring:
        std_threshold: 2.0

```

**Pipeline Summary**

1.  A poisoned model is trained using the same procedure as the original attack simulation.
    
2.  Validation accuracy is computed per class.
    
3.  Any class whose accuracy falls significantly below the global average (based on the specified standard deviation threshold) is flagged as suspicious.
    
4.  This analysis is diagnostic: no data is removed or model retrained.
    

**Outputs**

-   `results/data_poisoning/label_flipping/per_class_monitoring_results.json`  
    Contains per-class accuracy, global mean/std, and flagged classes.
    
-   `results/data_poisoning/label_flipping/per_class_monitoring_report.md`  
    Includes tabular summaries and insights into the most affected classes.

----------

### 3.3 Robust Loss

**Description**  
Robust loss functions aim to reduce the influence of noisy or adversarial samples by modifying how the model penalizes errors during training. Instead of relying on standard cross-entropy, alternative loss formulations are used to suppress the gradient impact of outliers.

**Applicable to:**

-   `label_flipping` (Data Poisoning)
    
-   `clean_label` (Data Poisoning)
    

**Configuration Parameters**  
Users can select one of the following robust loss strategies:

-   `type`:
    
    -   `"gce"` — Generalized Cross Entropy
        
    -   `"symmetric_cross_entropy"` — Combines CE and reverse-CE
        
    -   `"label_smoothing"` — Distributes target probability mass across all classes
        

YAML configuration example:

```yaml
defense_config:
  data_poisoning:
    clean_label:
      robust_loss:
        type: "gce"

```

**Pipeline Summary**

1.  The selected data poisoning attack is re-applied to regenerate the poisoned training set.
    
2.  A model is trained on the poisoned data using the chosen robust loss instead of standard cross-entropy.
    
3.  Evaluation is performed on a clean test set.
    

**Outputs**

-   `results/data_poisoning/{attack}/robust_loss_results.json`  
    Includes defense parameters, final accuracy, and per-class breakdown.
    
-   `results/data_poisoning/{attack}/robust_loss_report.md`  
    Describes the loss function used and presents performance metrics.
    

Note: This method does not remove samples; instead, it reduces their harmful impact during gradient updates.

----------

### 3.4 Differential Privacy Training

**Description**  
Differentially Private (DP) training introduces calibrated noise into the gradient updates to protect the contribution of individual data points. By applying DP-SGD via the Opacus library, this defense limits the ability of poisoned samples to dominate model learning, while also providing a formal privacy guarantee (ε, δ).

**Applicable to:**

-   `label_flipping` (Data Poisoning)
    
-   `clean_label` (Data Poisoning)
    

**Configuration Parameters**

-   `epsilon`: Target privacy budget (e.g., 2.0)
    
-   `delta`: Failure probability (e.g., 1e-5)
    
-   `clip_norm`: Gradient clipping norm for individual samples
    

YAML configuration example:

```yaml
defense_config:
  data_poisoning:
    label_flipping:
      dp_training:
        epsilon: 2.0
        delta: 1e-5
        clip_norm: 1.0

```

**Pipeline Summary**

1.  The poisoned training dataset is regenerated based on the selected attack.
    
2.  A model is trained using Opacus’ `make_private_with_epsilon`, which:
    
    -   Applies per-sample gradient clipping,
        
    -   Adds Gaussian noise to gradients,
        
    -   Computes the total privacy budget spent.
        
3.  BatchNorm layers are automatically replaced with GroupNorm to ensure compatibility.
    
4.  The trained model is evaluated on the test set.
    

**Outputs**

-   `results/data_poisoning/{attack}/dp_training_results.json`  
    Includes accuracy, ε spent, defense parameters, and per-class metrics.
    
-   `results/data_poisoning/{attack}/dp_training_report.md`  
    Summarizes the privacy settings, effectiveness, and accuracy results.
    

**Notes**

-   Secure RNG is disabled by default for performance but can be re-enabled for production use.
    
-   If BatchNorm is present, it is automatically converted for DP compliance.
    

----------

### 3.5 Provenance Tracking

**Description**  
Provenance tracking is a loss-based detection technique that identifies suspicious samples by analyzing their training behavior. It assumes that poisoned or mislabeled samples will yield unusually high losses when evaluated using a clean model. This defense re-applies the clean-label attack and tracks training loss per sample or batch to remove outliers.

**Applicable to:**

-   `clean_label` (Data Poisoning)
    

**Configuration Parameters**

-   `granularity`:
    
    -   `"sample"` — Removes samples with individual loss significantly above the mean
        
    -   `"batch"` — Removes entire batches whose average loss deviates from the global average
        
-   `batch_size`: Number of samples per batch used during loss computation (default = 64)
    

YAML configuration example:

```yaml
defense_config:
  data_poisoning:
    clean_label:
      provenance_tracking:
        granularity: "sample"
        batch_size: 64

```

**Pipeline Summary**

1.  A clean helper model is loaded from Module 2 (pretrained on the clean dataset).
    
2.  The clean-label attack is re-applied to create the poisoned dataset.
    
3.  The helper model computes training loss per sample or per batch.
    
4.  Samples/batches that exceed the threshold (mean + 2σ) are removed.
    
5.  A new model is trained on the cleaned data and evaluated.
    

**Outputs**

-   `results/data_poisoning/clean_label/provenance_tracking_results.json`  
    Contains number of removed samples, accuracy metrics, and paths to visual examples.
    
-   `results/data_poisoning/clean_label/provenance_tracking_report.md`  
    Includes tabular metrics and up to 5 visual examples.
    
-   `results/data_poisoning/clean_label/provenance_examples/`  
    Folder with saved images of removed samples.
    

----------

### 3.6 Influence Functions

**Description**  
This defense estimates how much each training sample influences the model’s loss on a clean test point. By computing gradient similarity between training and test samples, the method identifies and removes training points that are highly influential in a suspicious way — a signature often found in poisoned data.

**Applicable to:**

-   `clean_label` (Data Poisoning)
    

**Configuration Parameters**

-   `method`:
    
    -   `"grad_influence"` — Computes dot product between gradients (∇ℓᵗ ⋅ ∇ℓᵢ)
        
    -   `"hessian_inverse"` — Placeholder for future full influence computation (not yet implemented)
        
-   `sample_size`: Number of test samples to compare against (default: 500)
    

YAML configuration example:

```yaml
defense_config:
  data_poisoning:
    clean_label:
      influence_functions:
        method: "grad_influence"
        sample_size: 500

```

**Pipeline Summary**

1.  Loads a clean helper model previously trained in Module 2.
    
2.  Re-applies the clean-label attack to poison the training set.
    
3.  Computes the gradient of the loss for each poisoned sample and compares it to a subset of clean test samples.
    
4.  Calculates influence scores via dot product between gradients.
    
5.  Removes the top-N most influential training points.
    
6.  Retrains the model on the cleaned dataset and evaluates it.
    

**Outputs**

-   `results/data_poisoning/clean_label/influence_functions_results.json`  
    Includes influence ranking, number of samples removed, accuracy metrics, and parameters used.
    
-   `results/data_poisoning/clean_label/influence_functions_report.md`  
    Summarizes influence scores and final performance.
    
-   `results/data_poisoning/clean_label/influence_examples/`  
    Folder with visual examples of the most influential (and removed) training samples.
    

----------


### 3.7 Activation Clustering

**Description**  
Activation Clustering is a defense based on the hypothesis that poisoned samples introduced via backdoor attacks tend to form distinct clusters in the activation space of the model. By applying unsupervised clustering (e.g., K-Means) to neuron activations, this defense isolates and removes suspicious clusters.

**Applicable to:**

-   `static_patch` (Backdoor)
    
-   `learned_trigger` (Backdoor)
    

**Configuration Parameters**

-   `num_clusters`: Number of clusters used in K-Means (typically 2)
    

Example YAML configuration:

```yaml
defense_config:
  backdoor:
    static_patch:
      activation_clustering:
        num_clusters: 2

```

**Pipeline Summary**

1.  The backdoor attack is re-applied to poison the training dataset.
    
2.  A model is trained on the poisoned dataset.
    
3.  Activations from the last linear layer are extracted for all training samples.
    
4.  K-Means clustering is applied to the activations.
    
5.  The smallest cluster (by sample count) is considered suspicious and removed.
    
6.  A new model is retrained on the cleaned dataset and evaluated.
    

**Outputs**

-   `results/backdoor/{attack_type}/activation_clustering_results.json`  
    Contains accuracy, parameters used, and removed indices.
    
-   `results/backdoor/{attack_type}/activation_clustering_report.md`  
    Markdown report with summary metrics and visual examples.
    
-   `results/backdoor/{attack_type}/activation_removed/`  
    Folder with up to 5 saved images of removed examples.
    

----------

### 3.8 Spectral Signatures

**Description**  
Spectral Signatures is a statistical defense that identifies poisoned samples by analyzing the spectral properties of neural activations. It assumes that backdoored data introduces a consistent direction in the feature space, detectable via singular value decomposition (SVD). Outliers along the dominant singular vector are flagged as suspicious.

**Applicable to:**

-   `static_patch` (Backdoor)
    
-   `learned_trigger` (Backdoor)
    

**Configuration Parameters**

-   `threshold`: Quantile threshold used to filter out high-projection outliers (default: 0.9)
    

Example YAML configuration:

```yaml
defense_config:
  backdoor:
    learned_trigger:
      spectral_signatures:
        threshold: 0.9

```

**Pipeline Summary**

1.  The backdoor attack is re-applied to generate the poisoned training dataset.
    
2.  A model is trained on this poisoned data.
    
3.  Activations from a linear layer (typically penultimate) are collected for all training samples.
    
4.  Activations are grouped by class and centered.
    
5.  SVD is applied to each class separately.
    
6.  Samples with high projections onto the top singular vector (above the threshold) are removed.
    
7.  A clean model is retrained and evaluated.
    

**Outputs**

-   `results/backdoor/{attack_type}/spectral_signatures_results.json`  
    Accuracy, number of removed samples, and per-class metrics.
    
-   `results/backdoor/{attack_type}/spectral_signatures_report.md`  
    Markdown report with configuration, summary, and per-class histograms.
    
-   `results/backdoor/{attack_type}/spectral_histograms/`  
    Histogram plots of projection magnitudes for each class.
    

----------

### 3.9 Anomaly Detection

**Description**  
This defense identifies backdoored samples as statistical outliers in the activation space. It uses unsupervised anomaly detection algorithms — specifically Isolation Forest and Local Outlier Factor (LOF) — to detect and remove suspicious data points from the poisoned training set.

**Applicable to:**

-   `static_patch` (Backdoor)
    
-   `learned_trigger` (Backdoor)
    

**Configuration Parameters**

-   `type`: The anomaly detection method to use:
    
    -   `"isolation_forest"` — Tree-based model for high-dimensional anomaly detection.
        
    -   `"lof"` — Density-based method comparing local neighborhoods.
        

Example YAML configuration:

```yaml
defense_config:
  backdoor:
    static_patch:
      anomaly_detection:
        type: "isolation_forest"

```

**Pipeline Summary**

1.  The backdoor attack is re-applied to create a poisoned training dataset.
    
2.  A model is trained to extract high-level activations.
    
3.  Activations from a linear layer are collected across the dataset.
    
4.  The selected anomaly detection algorithm (IF or LOF) is applied to the activation vectors.
    
5.  Detected outliers are removed from the dataset.
    
6.  A new model is trained on the cleaned data and evaluated.
    

**Outputs**

-   `results/backdoor/{attack_type}/anomaly_detection_results.json`  
    Contains overall and per-class accuracy, removed indices, and parameters used.
    
-   `results/backdoor/{attack_type}/anomaly_detection_report.md`  
    Includes detection summary and example visualizations (if applicable).

----------

### 3.10 Pruning

**Description**  
This defense reduces the influence of backdoor triggers by pruning neurons from the model. It assumes that certain neurons are overly activated by backdoor patterns and can be safely removed without significantly harming overall performance. The user can choose to prune neurons across all layers or restrict pruning to the final fully connected layer.

**Applicable to:**

-   `static_patch` (Backdoor)
    
-   `learned_trigger` (Backdoor)
    

**Configuration Parameters**

-   `pruning_ratio`: Fraction of neurons to remove (e.g., `0.2` for 20%).
    
-   `scope`: Target of pruning:
    
    -   `"all_layers"` — Apply pruning to all linear layers.
        
    -   `"last_layer_only"` — Prune only the final classification layer.
        

Example YAML configuration:

```yaml
defense_config:
  backdoor:
    learned_trigger:
      pruning:
        pruning_ratio: 0.2
        scope: "all_layers"

```

**Pipeline Summary**

1.  The backdoor attack is re-applied to poison the training data.
    
2.  A model is trained on the poisoned dataset.
    
3.  Neurons with the largest average absolute activations are identified.
    
4.  The selected proportion of those neurons are zeroed out (pruned).
    
5.  The pruned model is evaluated on a clean test set.
    

**Outputs**

-   `results/backdoor/{attack_type}/pruning_results.json`  
    Includes pruning parameters, accuracy metrics, number of neurons removed, and their indices.
    
-   `results/backdoor/{attack_type}/pruning_report.md`  
    Summarizes pruning decisions and performance metrics.

----------

### 3.11 Fine-Pruning

**Description**  
Fine-Pruning is a lightweight defense that focuses exclusively on the last classification layer. It aims to remove neurons that are abnormally activated by backdoor triggers, assuming that these neurons are the primary carriers of malicious behavior. Unlike full pruning, it targets only a small subset of neurons and optionally retrains the model afterward.

**Applicable to:**

-   `static_patch` (Backdoor)
    
-   `learned_trigger` (Backdoor)
    

**Configuration Parameters**

-   `pruning_ratio`: Proportion of neurons to prune from the last layer (e.g., `0.2` = 20%).
    

Example YAML configuration:

```yaml
defense_config:
  backdoor:
    static_patch:
      fine_pruning:
        pruning_ratio: 0.2

```

**Pipeline Summary**

1.  The corresponding backdoor attack is reapplied.
    
2.  A model is trained on the poisoned data.
    
3.  The last linear layer is analyzed to detect neurons with abnormally high activation patterns.
    
4.  The most suspicious neurons are pruned based on activation statistics.
    
5.  The pruned model is evaluated on a clean test set.
    

**Outputs**

-   `results/backdoor/{attack_type}/fine_pruning_results.json`  
    Includes pruning ratio, indices of removed neurons, layer name, and accuracy metrics.
    
-   `results/backdoor/{attack_type}/fine_pruning_report.md`  
    Summarizes the pruning process and provides performance breakdown.
   

----------

### 3.12 Model Inspection

**Description**  
Model Inspection is a diagnostic defense that examines the weights of selected layers after training on potentially poisoned data. The method assumes that backdoor behavior often manifests as abnormal weight distributions in certain layers — particularly in the form of extreme magnitudes or variance. This defense does not modify the model but helps highlight suspicious internal patterns.

**Applicable to:**

-   `static_patch` (Backdoor)
    
-   `learned_trigger` (Backdoor)
    

**Configuration Parameters**

-   `layers`: List of layer names to inspect (e.g., `"fc1", "fc2"`).  
    These must match the names used in the model's architecture.
    

Example YAML configuration:

```yaml
defense_config:
  backdoor:
    static_patch:
      model_inspection:
        layers: ["fc1", "fc2"]

```

**Pipeline Summary**

1.  The backdoor attack is reapplied and the model is trained on the poisoned dataset.
    
2.  The weights of specified layers are extracted and analyzed.
    
3.  For each layer:
    
    -   Mean, standard deviation, and maximum absolute value are computed.
        
    -   Histograms of weight values are saved.
        
4.  Layers with high standard deviation or extreme weight magnitudes are flagged as suspicious.
    

**Outputs**

-   `results/backdoor/{attack_type}/model_inspection_results.json`  
    Includes accuracy, per-class metrics, stats for each inspected layer, and list of flagged layers.
    
-   `results/backdoor/{attack_type}/model_inspection_report.md`  
    Markdown report including summary metrics and visualizations.
    
-   `results/backdoor/{attack_type}/inspection_histograms/`  
    Contains weight histograms for each inspected layer.
    
    


## 4. Integration with Other Modules

Module 4 integrates tightly with the rest of the Safe-DL framework. It relies on previously generated artifacts and produces outputs that are essential for later stages in the pipeline.

### 4.1 Upstream Dependencies

-   **Module 1 — Threat Modeling**  
    The selected profile file (`.yaml`) defines the dataset, model architecture, and relevant attack vectors. Module 4 uses this profile to:
    
    -   Load the correct dataset and model;
        
    -   Access attack-specific parameters under `attack_overrides`;
        
    -   Apply only the defenses relevant to the specified threat model.
        
-   **Module 2 — Attack Simulation**  
    Defenses simulate the same attacks as defined in Module 2. For some defenses (e.g., `data_cleaning`, `robust_loss`), the poisoned dataset is regenerated internally based on `attack_overrides`.  
    Additionally, pretrained clean models from Module 2 are reused by:
    
    -   `provenance_tracking`
        
    -   `influence_functions`
        
-   **Module 3 — Risk Analysis**  
    Module 4 reads the recommended defense strategies per attack from the `risk_analysis.json` file. During the setup phase, the user can accept or override these recommendations. The selected defenses and parameters are stored in `defense_config`.
    

### 4.2 Downstream Outputs

The results generated in this module — including defense metrics, reports, and filtered datasets — are essential for:

-   **Module 5 — Comparative Evaluation**  
    Where multiple defenses may be compared across different attack types.
    
-   **Module 6 — Decision Support**  
    Where post-defense risk is reassessed to help guide deployment readiness.
    

All defense results are stored in the `results/` directory under structured paths like:

```
results/data_poisoning/label_flipping/robust_loss_results.json
results/data_poisoning/clean_label/provenance_tracking_report.md

```

These files include all necessary information to trace, reproduce, or analyze defense behavior independently.

----------

## 5. Summary and Future Work

This module has implemented a wide range of defense mechanisms covering multiple adversarial attack vectors. In particular, Module 4 now provides full support for:

-   **Data Poisoning Attacks**, including `label_flipping` and `clean_label`, with defenses such as:
    
    -   Data Cleaning
        
    -   Robust Loss
        
    -   Per-Class Monitoring
        
    -   Influence Functions
        
    -   Provenance Tracking
        
    -   Differential Privacy Training
        
-   **Backdoor Attacks**, including `static_patch` and `learned_trigger`, with defenses such as:
    
    -   Activation Clustering
        
    -   Spectral Signatures
        
    -   Anomaly Detection (LOF, Isolation Forest)
        
    -   Pruning
        
    -   Fine-Pruning
        
    -   Model Inspection
        

These defenses can be configured and executed via structured YAML profiles, enabling reproducibility and transparency across all experiments.

Each defense follows a modular pipeline, allowing for attack re-simulation, model training, mitigation application, and post-defense evaluation. Results are saved in a standardized format to facilitate comparison, visualization, and downstream analysis.

### Future Work

With backdoor and poisoning defenses now fully supported, upcoming extensions to Module 4 will include:

-   **Defenses for Evasion Attacks** (e.g., adversarial examples generated via FGSM, PGD, or black-box methods).
    
-   **Defense Orchestration**, where multiple strategies are combined adaptively based on threat severity and model behavior.
    
-   **Interactive Evaluation Dashboards**, integrating results from Modules 4–6 to support deployment decisions.
    

This extensible architecture positions Safe-DL as a robust framework for adversarial threat mitigation in deep learning pipelines.



