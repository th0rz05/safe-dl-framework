
# Module 4 — Defense Application

This module is responsible for applying defense mechanisms based on the threat profile previously defined and the attacks simulated in Module 2. Using the risk analysis performed in Module 3, it guides the selection, configuration, and execution of defenses to mitigate the effects of adversarial attacks on deep learning models.

## Table of Contents

- [1. Introduction](#1-introduction)
- [2. Setup Phase](#2-setup-phase)
  * [2.1 Loading the Threat Profile](#21-loading-the-threat-profile)
  * [2.2 Interactive Defense Selection](#22-interactive-defense-selection)
- [3. Implemented Defenses](#3-implemented-defenses)
  * [3.1 Data Cleaning](#31-data-cleaning)
  * [3.2 Per-Class Monitoring](#32-per-class-monitoring)
  * [3.3 Robust Loss Functions](#33-robust-loss-functions)
  * [3.4 Differentially Private Training](#34-differentially-private-training)
  * [3.5 Provenance Tracking](#35-provenance-tracking)
  * [3.6 Influence Functions](#36-influence-functions)
  * [3.7 Activation Clustering](#37-activation-clustering)
  * [3.8 Spectral Signatures](#38-spectral-signatures)
  * [3.9 Anomaly Detection](#39-anomaly-detection)
  * [3.10 Pruning](#310-pruning)
  * [3.11 Fine-Pruning](#311-fine-pruning)
  * [3.12 Model Inspection](#312-model-inspection)
  * [3.13 Adversarial Training](#313-adversarial-training)
  * [3.14 Randomized Smoothing](#314-randomized-smoothing)
  * [3.15 Gradient Masking](#315-gradient-masking)
  * [3.16 JPEG Preprocessing](#316-jpeg-preprocessing)
- [4. Integration with Other Modules](#4-integration-with-other-modules)
  * [4.1 Upstream Dependencies](#41-upstream-dependencies)
  * [4.2 Downstream Outputs](#42-downstream-outputs)
- [5. Summary and Future Work](#5-summary-and-future-work)
  * [5.1 Data Poisoning Attacks](#51-data-poisoning-attacks)
  * [5.2 Backdoor Attacks](#52-backdoor-attacks)
  * [5.3 Evasion Attacks](#53-evasion-attacks)
  * [5.4 Future Work](#54-future-work)

## 1. Introduction

As deep learning models are increasingly exposed to adversarial threats in real-world applications, the ability to assess and mitigate these threats is essential. Module 4 addresses this need by systematically applying defense strategies tailored to the specific attacks identified in previous phases of the Safe-DL framework.

Each defense is applied in a structured pipeline that re-simulates the threat scenario, applies the mitigation, retrains the model, and evaluates post-defense performance. This ensures a consistent and comparative assessment of the effectiveness of each strategy.

The module supports both data-centric and model-centric defenses, and is designed to be extensible to various types of attacks, including poisoning, evasion, and backdoor threats. Configuration and control are managed via YAML profiles, ensuring reproducibility and transparency throughout the process

----------


## 2. Setup Phase

Before any defenses are applied, the user must select a previously generated threat profile and confirm which defense strategies to apply. This setup step ensures that defenses are aligned with the attack vectors detected in Module 2 and the risk assessments generated in Module 3.

### 2.1 Loading the Threat Profile

The user begins by selecting a `.yaml` threat profile generated by Module 1. This profile encapsulates:

-   Dataset and model specifications;
    
-   Attack configurations and metrics (from Module 2);
    
-   Risk levels and suggested mitigations (from Module 3).
    

This file acts as the central control point for guiding the defense logic. Upon loading, the system extracts relevant metadata (such as attack types and targets) to determine which defenses are available and applicable.

### 2.2 Interactive Defense Selection

Once the profile is loaded, the framework presents a list of recommended defenses for each attack type, based on mappings defined in `defense_tags.py`. Users can choose to:

-   Accept the system’s recommendations;
    
-   Customize the selected defenses per attack;
    
-   Configure specific parameters for each method.
    

All choices are stored under the `defense_config` section of the profile file. This design allows for full traceability and reproducibility of the defense setup phase, as the exact configuration used during execution is saved and version-controlled
        

----------

## 3. Implemented Defenses

This section documents the defense mechanisms currently implemented in the Safe-DL framework. Each subsection corresponds to one defense and includes its logic, applicable attack types, configurable parameters, and output structure.

### 3.1 Data Cleaning

**Objective:**  
The goal of this defense is to remove suspicious or noisy training samples that may have been injected through data poisoning attacks, such as label flipping. This is achieved by analyzing the loss behavior of individual training samples and applying statistical or threshold-based filters to discard anomalous points.


----------

**Defense Pipeline:**

1.  **Attack Simulation:**  
    The selected data poisoning attack (e.g., label flipping) is applied to the training set according to the threat profile parameters.
    
2.  **Poisoned Model Training:**  
    A temporary model is trained on the poisoned training set for a few epochs. This model is used to extract per-sample loss statistics.
    
3.  **Sample Loss Analysis:**  
    Each training sample is passed through the model to compute its individual loss using cross-entropy. Two cleaning methods are supported:
    
    -   `loss_filtering`: Retains samples with loss below a threshold relative to the mean.
        
    -   `outlier_detection`: Retains samples with a z-score within a configurable standard deviation.
        
4.  **Cleaned Model Training:**  
    A new model is trained from scratch using only the retained, presumably clean, subset of the training data.
    
5.  **Evaluation and Reporting:**  
    The cleaned model is evaluated on the untouched (clean) test set. Up to 5 removed examples are visualized and saved. A Markdown and JSON report is generated.
    

----------

**Output JSON Format:**

```json
{
  "defense": "data_cleaning",
  "attack": "label_flipping",
  "accuracy_clean": 0.8812,
  "per_class_accuracy_clean": {
    "airplane": 0.89,
    "automobile": 0.87,
    ...
  }
  "cleaning_params": {
    "method": "loss_filtering",
    "threshold": 1.5
  },
  "num_removed": 217,
  "example_removed": [
    {
      "index": 1324,
      "original_label": 4,
      "original_label_name": "deer",
      "image_path": "cleaned_examples/removed_1324_4.png"
    }
  ]
}

```



----------

### 3.2 Per-Class Monitoring

**Objective:**  
This lightweight defense aims to detect anomalies caused by data poisoning by analyzing the distribution of per-class accuracies. The assumption is that a successful data poisoning attack will disproportionately degrade performance in specific target classes, leading to an unusual drop in accuracy for those classes.


----------

**Defense Pipeline:**

1.  **Load Attack Metrics:**  
    The defense uses the results of the attack simulation phase (e.g., `label_flipping_metrics.json`) to analyze the per-class accuracy of the model after being trained on poisoned data.
    
2.  **Statistical Outlier Detection:**  
    The mean and standard deviation of all per-class accuracies are computed. Any class whose accuracy falls below `mean - (threshold × std)` is flagged as potentially affected by poisoning.
    
3.  **Reporting:**  
    All flagged classes are reported in both JSON and Markdown formats, alongside the global statistics used during detection. No retraining or model modification is performed in this defense — it serves only as a monitoring and analysis tool.
    

----------

**Output JSON Format:**

```json
{
  "defense": "per_class_monitoring",
  "attack": "label_flipping",
  "accuracy_mean": 0.8754,
  "accuracy_std": 0.0611,
  "threshold": 1.5,
  "flagged_classes": [
    { "class": "deer", "accuracy": 0.7021 },
    { "class": "frog", "accuracy": 0.6942 }
  ],
  "per_class_accuracy": {
    "airplane": 0.91,
    "automobile": 0.89,
    ...
  }
}

```

----------

>  This defense is particularly useful for early-stage detection of suspicious class-specific behavior before applying heavier techniques such as influence-based filtering or robust retraining.


----------

### 3.3 Robust Loss Functions

**Objective:**  
This defense strategy aims to mitigate the influence of poisoned or mislabeled data by replacing the standard cross-entropy loss with a more robust alternative during training. These loss functions are designed to be less sensitive to outliers or noisy labels, which are common in data poisoning scenarios.

----------

**Available Loss Functions:**

The framework currently supports the following robust loss functions:

-   **Generalized Cross Entropy (GCE)**
    
-   **Symmetric Cross Entropy (SCE)**
    
-   **Bootstrap Loss (Soft/Hard variants)**
    

The loss function is selected via the YAML profile under:

```yaml
defense_config:
  data_poisoning:
    label_flipping:
      robust_loss:
        loss_type: gce
        alpha: 0.2
        beta: null

```

Each loss function may include different hyperparameters, such as `alpha` (weight for robust term) or `beta` (weight for original cross-entropy component), depending on its formulation.

----------

**Defense Pipeline:**

1.  **Model Initialization:**  
    A clean model is initialized from the configuration in the YAML profile.
    
2.  **Robust Training:**  
    The model is trained from scratch on the poisoned dataset using the specified robust loss function instead of the default cross-entropy.
    
3.  **Evaluation:**  
    After training, the model is evaluated on the clean test set. The metrics include overall accuracy and per-class accuracy.
    

----------

**Output JSON Format:**

```json
{
  "defense": "robust_loss",
  "attack": "label_flipping",
  "loss_type": "gce",
  "alpha": 0.2,
  "accuracy_clean": 0.8695,
  "per_class_accuracy_clean": {
    "airplane": 0.91,
    "automobile": 0.85,
    ...
  }
}

```

----------

> This method avoids explicit filtering and instead enhances the model’s robustness through improved optimization criteria, making it an efficient and scalable defense for large datasets.

----------

### 3.4 Differentially Private Training 

**Objective:**  
This defense leverages differentially private stochastic gradient descent (DP-SGD) to train models with formal privacy guarantees. While its primary use is privacy preservation, DP training has also been shown to improve robustness against certain data poisoning attacks by reducing model sensitivity to individual training samples.

----------

**Configuration in Profile YAML:**

```yaml
defense_config:
  data_poisoning:
    label_flipping:
      dp_training:
        noise_multiplier: 1.0
        max_grad_norm: 1.0

```

-   `noise_multiplier`: Amount of Gaussian noise added to clipped gradients.
    
-   `max_grad_norm`: Gradient clipping threshold.
    

----------

**Defense Pipeline:**

1.  **Model Initialization:**  
    A fresh model is loaded from the configuration specified in the YAML threat profile.
    
2.  **DP-SGD Training:**  
    Training is conducted with Opacus, using per-sample gradient clipping and noise injection to enforce differential privacy. Training is done on the (potentially poisoned) dataset.
    
3.  **Evaluation:**  
    After training, the model is evaluated on the clean test set. Only clean accuracy is recorded.
    

----------

**Output JSON Format:**

```json
{
  "defense": "dp_training",
  "attack": "label_flipping",
  "accuracy_clean": 0.8412,
  "per_class_accuracy_clean": {
    "airplane": 0.84,
    "automobile": 0.86,
    ...
  },
  "dp_params": {
    "noise_multiplier": 1.0,
    "max_grad_norm": 1.0
  }
}

```

----------

>  DP-SGD introduces randomness that can reduce the model’s reliance on specific poisoned samples, thus mitigating the impact of data poisoning. However, it may also lower overall accuracy if the privacy budget is too strict.


----------

### 3.5 Provenance Tracking

**Objective:**  
This defense uses metadata about the origin and context of training data (i.e., provenance) to detect and filter out potentially untrusted or suspicious samples. It assumes that poisoned samples often originate from unverified or anomalous sources.

----------

**Configuration in Profile YAML:**

```yaml
defense_config:
  data_poisoning:
    label_flipping:
      provenance_tracking:
        trusted_sources: ["source1", "source2"]

```

-   `trusted_sources`: A list of identifiers marking trusted data origins. All samples from other sources will be excluded from training.
    

----------

**Defense Pipeline:**

1.  **Metadata Extraction:**  
    Each sample in the dataset is assumed to contain metadata (e.g., `sample.source`) indicating its origin.
    
2.  **Filtering Step:**  
    All training samples not originating from the configured `trusted_sources` are removed.
    
3.  **Model Training:**  
    A new model is trained exclusively on data from trusted sources.
    
4.  **Evaluation:**  
    The model is evaluated on the clean test set. Only clean accuracy is recorded.
    

----------

**Output JSON Format:**

```json
{
  "defense": "provenance_tracking",
  "attack": "label_flipping",
  "accuracy_clean": 0.8519,
  "per_class_accuracy_clean": {
    "airplane": 0.87,
    "automobile": 0.82,
    ...
  },
  "trusted_sources_used": ["source1", "source2"],
  "samples_retained": 3821
}

```

----------

>  Provenance filtering is particularly effective in scenarios where poisoned data can be traced back to unverified or untrusted contributors (e.g., crowdsourced data pipelines). However, this method requires metadata to be reliably available and accurate.

----------

### 3.6 Influence Functions

**Objective:**  
This defense leverages influence functions to identify training samples that have a disproportionately large effect on the model's predictions. By removing the most harmful samples (i.e., those with negative influence), it mitigates the impact of data poisoning attacks.


----------

**Configuration in Profile YAML:**

```yaml
defense_config:
  data_poisoning:
    label_flipping:
      influence_functions:
        num_samples_to_remove: 200

```

-   `num_samples_to_remove`: Number of training samples with the most negative influence to discard.
    

----------

**Defense Pipeline:**

1.  **Initial Training:**  
    Train a temporary model on the (possibly poisoned) dataset.
    
2.  **Influence Score Estimation:**  
    For each training sample, compute its influence on the loss of a small validation set using approximations of the Hessian-vector product.
    
3.  **Sample Filtering:**  
    Remove the `num_samples_to_remove` samples with the lowest (most negative) influence scores.
    
4.  **Final Training:**  
    Retrain the model on the cleaned dataset.
    
5.  **Evaluation:**  
    Evaluate the retrained model on the clean test set. Adversarial accuracy is omitted.
    

----------

**Output JSON Format:**

```json
{
  "defense": "influence_functions",
  "attack": "label_flipping",
  "accuracy_clean": 0.8447,
  "per_class_accuracy_clean": {
    "airplane": 0.87,
    "automobile": 0.82,
    ...
  },
  "samples_removed": 200
}

```

----------

>  Influence functions are powerful for identifying subtle poisoning effects, especially when the attacker poisons data without drastically changing the data distribution. However, this technique can be computationally intensive on large datasets or deep models.

----------


### 3.7 Activation Clustering

**Description**  
Activation Clustering is a defense based on the hypothesis that poisoned samples introduced via backdoor attacks tend to form distinct clusters in the activation space of the model. By applying unsupervised clustering (e.g., K-Means) to neuron activations, this defense isolates and removes suspicious clusters.

**Applicable to:**

-   `static_patch` (Backdoor)
    
-   `learned_trigger` (Backdoor)
    

**Configuration Parameters**

-   `num_clusters`: Number of clusters used in K-Means (typically 2)
    

Example YAML configuration:

```yaml
defense_config:
  backdoor:
    static_patch:
      activation_clustering:
        num_clusters: 2

```

**Pipeline Summary**

1.  The backdoor attack is re-applied to poison the training dataset.
    
2.  A model is trained on the poisoned dataset.
    
3.  Activations from the last linear layer are extracted for all training samples.
    
4.  K-Means clustering is applied to the activations.
    
5.  The smallest cluster (by sample count) is considered suspicious and removed.
    
6.  A new model is retrained on the cleaned dataset and evaluated.
    

**Outputs**

-   `results/backdoor/{attack_type}/activation_clustering_results.json`  
    Contains accuracy, parameters used, and removed indices.
    
-   `results/backdoor/{attack_type}/activation_clustering_report.md`  
    Markdown report with summary metrics and visual examples.
    
-   `results/backdoor/{attack_type}/activation_removed/`  
    Folder with up to 5 saved images of removed examples.
    

----------

### 3.8 Spectral Signatures

**Description**  
Spectral Signatures is a statistical defense that identifies poisoned samples by analyzing the spectral properties of neural activations. It assumes that backdoored data introduces a consistent direction in the feature space, detectable via singular value decomposition (SVD). Outliers along the dominant singular vector are flagged as suspicious.

**Applicable to:**

-   `static_patch` (Backdoor)
    
-   `learned_trigger` (Backdoor)
    

**Configuration Parameters**

-   `threshold`: Quantile threshold used to filter out high-projection outliers (default: 0.9)
    

Example YAML configuration:

```yaml
defense_config:
  backdoor:
    learned_trigger:
      spectral_signatures:
        threshold: 0.9

```

**Pipeline Summary**

1.  The backdoor attack is re-applied to generate the poisoned training dataset.
    
2.  A model is trained on this poisoned data.
    
3.  Activations from a linear layer (typically penultimate) are collected for all training samples.
    
4.  Activations are grouped by class and centered.
    
5.  SVD is applied to each class separately.
    
6.  Samples with high projections onto the top singular vector (above the threshold) are removed.
    
7.  A clean model is retrained and evaluated.
    

**Outputs**

-   `results/backdoor/{attack_type}/spectral_signatures_results.json`  
    Accuracy, number of removed samples, and per-class metrics.
    
-   `results/backdoor/{attack_type}/spectral_signatures_report.md`  
    Markdown report with configuration, summary, and per-class histograms.
    
-   `results/backdoor/{attack_type}/spectral_histograms/`  
    Histogram plots of projection magnitudes for each class.
    

----------

### 3.9 Anomaly Detection

**Description**  
This defense identifies backdoored samples as statistical outliers in the activation space. It uses unsupervised anomaly detection algorithms — specifically Isolation Forest and Local Outlier Factor (LOF) — to detect and remove suspicious data points from the poisoned training set.

**Applicable to:**

-   `static_patch` (Backdoor)
    
-   `learned_trigger` (Backdoor)
    

**Configuration Parameters**

-   `type`: The anomaly detection method to use:
    
    -   `"isolation_forest"` — Tree-based model for high-dimensional anomaly detection.
        
    -   `"lof"` — Density-based method comparing local neighborhoods.
        

Example YAML configuration:

```yaml
defense_config:
  backdoor:
    static_patch:
      anomaly_detection:
        type: "isolation_forest"

```

**Pipeline Summary**

1.  The backdoor attack is re-applied to create a poisoned training dataset.
    
2.  A model is trained to extract high-level activations.
    
3.  Activations from a linear layer are collected across the dataset.
    
4.  The selected anomaly detection algorithm (IF or LOF) is applied to the activation vectors.
    
5.  Detected outliers are removed from the dataset.
    
6.  A new model is trained on the cleaned data and evaluated.
    

**Outputs**

-   `results/backdoor/{attack_type}/anomaly_detection_results.json`  
    Contains overall and per-class accuracy, removed indices, and parameters used.
    
-   `results/backdoor/{attack_type}/anomaly_detection_report.md`  
    Includes detection summary and example visualizations (if applicable).

----------

### 3.10 Pruning

**Description**  
This defense reduces the influence of backdoor triggers by pruning neurons from the model. It assumes that certain neurons are overly activated by backdoor patterns and can be safely removed without significantly harming overall performance. The user can choose to prune neurons across all layers or restrict pruning to the final fully connected layer.

**Applicable to:**

-   `static_patch` (Backdoor)
    
-   `learned_trigger` (Backdoor)
    

**Configuration Parameters**

-   `pruning_ratio`: Fraction of neurons to remove (e.g., `0.2` for 20%).
    
-   `scope`: Target of pruning:
    
    -   `"all_layers"` — Apply pruning to all linear layers.
        
    -   `"last_layer_only"` — Prune only the final classification layer.
        

Example YAML configuration:

```yaml
defense_config:
  backdoor:
    learned_trigger:
      pruning:
        pruning_ratio: 0.2
        scope: "all_layers"

```

**Pipeline Summary**

1.  The backdoor attack is re-applied to poison the training data.
    
2.  A model is trained on the poisoned dataset.
    
3.  Neurons with the largest average absolute activations are identified.
    
4.  The selected proportion of those neurons are zeroed out (pruned).
    
5.  The pruned model is evaluated on a clean test set.
    

**Outputs**

-   `results/backdoor/{attack_type}/pruning_results.json`  
    Includes pruning parameters, accuracy metrics, number of neurons removed, and their indices.
    
-   `results/backdoor/{attack_type}/pruning_report.md`  
    Summarizes pruning decisions and performance metrics.

----------

### 3.11 Fine-Pruning

**Description**  
Fine-Pruning is a lightweight defense that focuses exclusively on the last classification layer. It aims to remove neurons that are abnormally activated by backdoor triggers, assuming that these neurons are the primary carriers of malicious behavior. Unlike full pruning, it targets only a small subset of neurons and optionally retrains the model afterward.

**Applicable to:**

-   `static_patch` (Backdoor)
    
-   `learned_trigger` (Backdoor)
    

**Configuration Parameters**

-   `pruning_ratio`: Proportion of neurons to prune from the last layer (e.g., `0.2` = 20%).
    

Example YAML configuration:

```yaml
defense_config:
  backdoor:
    static_patch:
      fine_pruning:
        pruning_ratio: 0.2

```

**Pipeline Summary**

1.  The corresponding backdoor attack is reapplied.
    
2.  A model is trained on the poisoned data.
    
3.  The last linear layer is analyzed to detect neurons with abnormally high activation patterns.
    
4.  The most suspicious neurons are pruned based on activation statistics.
    
5.  The pruned model is evaluated on a clean test set.
    

**Outputs**

-   `results/backdoor/{attack_type}/fine_pruning_results.json`  
    Includes pruning ratio, indices of removed neurons, layer name, and accuracy metrics.
    
-   `results/backdoor/{attack_type}/fine_pruning_report.md`  
    Summarizes the pruning process and provides performance breakdown.
   

----------

### 3.12 Model Inspection

**Description**  
Model Inspection is a diagnostic defense that examines the weights of selected layers after training on potentially poisoned data. The method assumes that backdoor behavior often manifests as abnormal weight distributions in certain layers — particularly in the form of extreme magnitudes or variance. This defense does not modify the model but helps highlight suspicious internal patterns.

**Applicable to:**

-   `static_patch` (Backdoor)
    
-   `learned_trigger` (Backdoor)
    

**Configuration Parameters**

-   `layers`: List of layer names to inspect (e.g., `"fc1", "fc2"`).  
    These must match the names used in the model's architecture.
    

Example YAML configuration:

```yaml
defense_config:
  backdoor:
    static_patch:
      model_inspection:
        layers: ["fc1", "fc2"]

```

**Pipeline Summary**

1.  The backdoor attack is reapplied and the model is trained on the poisoned dataset.
    
2.  The weights of specified layers are extracted and analyzed.
    
3.  For each layer:
    
    -   Mean, standard deviation, and maximum absolute value are computed.
        
    -   Histograms of weight values are saved.
        
4.  Layers with high standard deviation or extreme weight magnitudes are flagged as suspicious.
    

**Outputs**

-   `results/backdoor/{attack_type}/model_inspection_results.json`  
    Includes accuracy, per-class metrics, stats for each inspected layer, and list of flagged layers.
    
-   `results/backdoor/{attack_type}/model_inspection_report.md`  
    Markdown report including summary metrics and visualizations.
    
-   `results/backdoor/{attack_type}/inspection_histograms/`  
    Contains weight histograms for each inspected layer.


----------

### 3.13 Adversarial Training

**Objective:**  
Adversarial Training is a proactive defense that retrains the model using a mix of clean and adversarial examples, making it more robust to inference-time attacks. This technique increases model resilience by simulating potential evasion attacks during training and adapting the model to handle such perturbations.

----------

**Configuration in YAML Profile:**

```yaml
defense_config:
  evasion:
    pgd:
      adversarial_training:
        attack_type: fgsm
        epsilon: 0.03
        mixed_with_clean: true

```

-   `attack_type`: Type of adversarial attack used to generate training-time adversarial examples (`fgsm`, `pgd`, etc.).
    
-   `epsilon`: Perturbation strength used for adversarial sample generation.
    
-   `mixed_with_clean`: Whether to mix clean and adversarial samples during training (`true` improves generalization).
    

----------

**Defense Pipeline:**

1.  **Base Model Initialization:**  
    A new model is initialized from scratch using the architecture defined in the threat profile.
    
2.  **Adversarial Example Generation:**  
    At each training step, adversarial examples are generated using the specified base attack (e.g., FGSM), with the configured `epsilon`.
    
3.  **Mixed Training:**  
    If `mixed_with_clean` is enabled, each batch is composed of 50% clean and 50% adversarial examples to preserve generalization.
    
4.  **Evaluation:**  
    After training, the model is evaluated both on the clean test set and a fresh batch of adversarial examples generated using the attack under evaluation (e.g., PGD). Metrics are logged to JSON and Markdown reports.
    

----------

**Output JSON Format:**

```json
{
  "defense_name": "adversarial_training",
  "evaluated_attack": "pgd",
  "accuracy_clean": 0.4439,
  "accuracy_adversarial": 0.1942,
  "per_class_accuracy_clean": {
    "airplane": 0.636,
    "automobile": 0.771,
    "bird": 0.197,
    "cat": 0.016,
    "deer": 0.429,
    "dog": 0.6,
    "frog": 0.476,
    "horse": 0.389,
    "ship": 0.535,
    "truck": 0.39
  },
  "per_class_accuracy_adversarial": {
    "3": 0.005,
    "8": 0.203,
    "0": 0.367,
    "6": 0.118,
    "1": 0.482,
    "9": 0.137,
    "5": 0.303,
    "7": 0.131,
    "4": 0.154,
    "2": 0.042
  },
  "parameters": {
    "epsilon": 0.03,
    "base_attack_used_for_training": "fgsm",
    "mixed_with_clean": true
  }
}

```

----------

> Adversarial Training significantly boosts robustness to known attacks but often leads to a trade-off in clean accuracy. The effectiveness of this defense depends heavily on the attack type and ε used during training, as well as whether clean examples are preserved.

----------

### 3.14 Randomized Smoothing

**Objective:**  
Randomized Smoothing is a certified defense technique that constructs a smoothed classifier by injecting Gaussian noise into the inputs. This method provides probabilistic robustness guarantees: if the base classifier’s prediction is stable under Gaussian perturbations, then the smoothed classifier is robust within a certain radius around the input.

----------

**Configuration in YAML Profile:**

```yaml
defense_config:
  evasion:
    spsa:
      randomized_smoothing:
        sigma: 0.25

```

-   `sigma`: Standard deviation of the Gaussian noise added to each input during smoothing. Higher values improve robustness but can degrade accuracy.
    

----------

**Defense Pipeline:**

1.  **Base Model Loading:**  
    A clean model trained without defenses is loaded from the threat profile.
    
2.  **Prediction with Noise Injection:**  
    For each test sample, multiple noisy versions are created by adding Gaussian noise with standard deviation `sigma`.
    
3.  **Majority Vote Classification:**  
    The final prediction for each sample is obtained via majority voting across multiple noisy versions, effectively stabilizing predictions under perturbations.
    
4.  **Evaluation:**  
    The smoothed classifier is evaluated both on clean and adversarial test sets. The same adversarial attack (e.g., SPSA) is reapplied to test robustness. Results are saved to JSON and Markdown.
    

----------

**Output JSON Format:**

```json
{
  "defense_name": "randomized_smoothing",
  "evaluated_attack": "spsa",
  "accuracy_clean": 0.4321,
  "accuracy_adversarial": 0.1823,
  "per_class_accuracy_clean": {
    "airplane": 0.59,
    "automobile": 0.73,
    "bird": 0.24,
    ...
  },
  "per_class_accuracy_adversarial": {
    "airplane": 0.34,
    "automobile": 0.51,
    "bird": 0.08,
    ...
  },
  "parameters": {
    "sigma": 0.25,
    "num_samples": 100,
    "certified": false
  }
}

```

> In this implementation, randomized smoothing is used purely as a stochastic defense — it does **not** provide certified radii. For formal certification (e.g., Cohen et al., 2019), a tighter integration would be needed with certified accuracy bounds and abstention mechanisms.


> Randomized Smoothing offers a simple yet effective way to improve robustness against black-box attacks such as NES or SPSA, particularly when combined with abstention or confidence calibration strategies.

----------

### 3.15 Gradient Masking

**Objective:**  
This defense aims to reduce the effectiveness of gradient-based and score-based adversarial attacks by introducing gradient obfuscation techniques. The central idea is to limit the model’s sensitivity to adversarial perturbations by dampening or distorting its gradients.

> **Note:** Gradient masking is not considered a robust long-term defense. It may hinder certain attacks like SPSA, FGSM, or PGD, but often fails against transfer-based or black-box methods.

----------

**Applicable to:**

-   Gradient-based and score-based evasion attacks (e.g., FGSM, PGD, SPSA)
    

----------

**Configuration Parameters**

The defense uses a single parameter:

-   `strength` — controls the intensity of gradient masking. A value between 0 and 1.
    

YAML example:

```yaml
defense_config:
  evasion:
    gradient_masking:
      strength: 0.5

```

----------

**Defense Pipeline:**

1.  **Masking Application:**  
    During both training and inference, gradients are suppressed or perturbed internally using the configured strength factor.
    
2.  **Training Phase:**  
    The model is trained on the clean dataset with masking applied.
    
3.  **Evaluation Phase:**  
    The trained model is evaluated on:
    
    -   Clean test set.
        
    -   Adversarial examples generated by the attack specified in the threat profile (e.g., SPSA).
        
4.  **Reporting:**  
    Results are logged in both JSON and Markdown formats. This includes accuracy metrics and the masking parameter used.
    

----------

**Output JSON Format:**

```json
{
  "defense": "gradient_masking",
  "attack": "spsa",
  "accuracy_clean": 0.6708,
  "accuracy_adversarial": 0.035,
  "per_class_accuracy_clean": {
    "airplane": 0.67,
    "automobile": 0.777,
    "bird": 0.624,
    "cat": 0.355,
    "deer": 0.634,
    "dog": 0.598,
    "frog": 0.693,
    "horse": 0.776,
    "ship": 0.856,
    "truck": 0.725
  },
  "per_class_accuracy_adversarial": {
    "airplane": 0.0,
    "automobile": 0.1429,
    "bird": 0.0,
    "cat": 0.0,
    "deer": 0.0,
    "dog": 0.0556,
    "frog": 0.0,
    "horse": 0.1111,
    "ship": 0.0357,
    "truck": 0.0476
  },
  "params": {
    "strength": 0.5
  }
}

```

----------

### 3.16 JPEG Preprocessing

**Objective:**  
JPEG Preprocessing is a simple yet effective defense that reduces the success rate of adversarial attacks by applying lossy compression to input images. This compression tends to eliminate subtle perturbations introduced by evasion attacks, especially score-based black-box attacks like NES and SPSA.

----------

**Applicable to:**

-   Score-based evasion attacks (e.g., SPSA, NES)
    
-   Particularly effective when adversarial perturbations are subtle and high-frequency
    

----------

**Configuration Parameters**

This defense requires one parameter:

-   `quality`: JPEG quality level for compression. Must be an integer between 1 and 100. Lower values increase compression (and distortion), while higher values preserve more of the original image.
    

YAML example:

```yaml
defense_config:
  evasion:
    jpeg_preprocessing:
      quality: 25

```

----------

**Defense Pipeline:**

1.  **JPEG Compression:**  
    Each input image (clean or adversarial) is passed through a JPEG encoder and then decoded before being fed into the model.
    
2.  **Training Phase:**  
    No retraining is required. This is a **test-time defense** — the model remains unchanged.
    
3.  **Evaluation Phase:**  
    The defense is evaluated on:
    
    -   Clean test set (after JPEG preprocessing)
        
    -   Adversarial test set (after JPEG preprocessing)
        
4.  **Reporting:**  
    Accuracy results are saved in both JSON and Markdown formats. The report includes class-level accuracy before and after preprocessing, and the `quality` used.
    

----------

**Example Output JSON Format:**

```json
{
  "defense": "jpeg_preprocessing",
  "attack": "spsa",
  "accuracy_clean": 0.6842,
  "accuracy_adversarial": 0.071,
  "per_class_accuracy_clean": {
    "airplane": 0.68,
    "automobile": 0.733,
    "bird": 0.602,
    "cat": 0.366,
    "deer": 0.642,
    "dog": 0.602,
    "frog": 0.711,
    "horse": 0.773,
    "ship": 0.848,
    "truck": 0.737
  },
  "per_class_accuracy_adversarial": {
    "airplane": 0.0,
    "automobile": 0.1905,
    "bird": 0.0,
    "cat": 0.0,
    "deer": 0.0,
    "dog": 0.0556,
    "frog": 0.0,
    "horse": 0.1905,
    "ship": 0.0714,
    "truck": 0.0952
  },
  "params": {
    "quality": 25
  }
}

```

----------


## 4. Integration with Other Modules

Module 4 integrates tightly with the rest of the Safe-DL framework. It relies on previously generated artifacts and produces outputs that are essential for later stages in the pipeline.

### 4.1 Upstream Dependencies

-   **Module 1 — Threat Modeling**  
    The selected profile file (`.yaml`) defines the dataset, model architecture, and relevant attack vectors. Module 4 uses this profile to:
    
    -   Load the correct dataset and model;
        
    -   Access attack-specific parameters under `attack_overrides`;
        
    -   Apply only the defenses relevant to the specified threat model.
        
-   **Module 2 — Attack Simulation**  
    Defenses simulate the same attacks as defined in Module 2. For some defenses (e.g., `data_cleaning`, `robust_loss`), the poisoned dataset is regenerated internally based on `attack_overrides`.  
    Additionally, pretrained clean models from Module 2 are reused by:
    
    -   `provenance_tracking`
        
    -   `influence_functions`
    
    For evasion defenses (e.g., `adversarial_training`, `gradient_masking`, `jpeg_preprocessing`), the adversarial test sets generated in Module 2 are reused, ensuring consistent evaluation.
        
-   **Module 3 — Risk Analysis**  
    Module 4 reads the recommended defense strategies per attack from the `risk_analysis.json` file. During the setup phase, the user can accept or override these recommendations. The selected defenses and parameters are stored in `defense_config`.
   

----------

### 4.2 Downstream Outputs

The results generated in this module — including defense metrics, visual reports, and filtered datasets — are essential for:

-   **Module 5 — Comparative Evaluation**  
    Where defenses are compared based on standardized metrics across different attack types.
    
-   **Module 6 — Decision Support**  
    Where post-defense risk is reassessed to support deployment decisions and guide model certification.
    

All defense results are stored under the `results/` directory using a structured hierarchy:

```
results/data_poisoning/label_flipping/data_cleaning_results.json
results/backdoor/static_patch/activation_clustering_report.md
results/evasion/spsa/gradient_masking_results.json

```


Each defense outputs its results using a consistent schema, including **attack-specific metrics**:

-   `accuracy_clean`: model accuracy on unmodified test data after defense application.
    
-   **For Evasion defenses, `accuracy_adversarial`**: model accuracy on adversarially perturbed inputs after defense application.
    
-   **For Backdoor defenses, `asr_after_defense`**: Attack Success Rate (ASR) of the backdoor on trigger-applied inputs after defense application.
    
-   `per_class_accuracy_clean`: per-class breakdown on clean inputs.
    
-   **For Evasion defenses, `per_class_accuracy_adversarial`**: per-class breakdown on adversarial inputs.
    
-   **For Backdoor defenses, `per_original_class_asr`**: ASR breakdown per original class for backdoor attacks.
    
-   `params`: dictionary of defense-specific configuration parameters.
    
-   **Additional defense-specific fields**: such as `num_removed` (number of removed samples), `removed_indices`, `example_removed` (paths to visual examples), `anomalous_classes` (for data poisoning), `pruned_params_fraction` (for pruning), or `suspicious_layers` (for model inspection), may be included depending on the defense type.

This output structure enables traceability, comparability, and downstream analysis for all implemented defenses.

----------

## 5. Summary and Future Work

This module has implemented a comprehensive set of defense mechanisms covering multiple adversarial attack vectors. As of the current version, **Module 4** fully supports:

----------

###  5.1 Data Poisoning Attacks

(e.g., `label_flipping`, `clean_label`)

-   **Data Cleaning**
    
-   **Robust Loss Training**
    
-   **Per-Class Monitoring**
    
-   **Influence Functions**
    
-   **Provenance Tracking**
    
-   **Differential Privacy Training**
    

----------

### 5.2 Backdoor Attacks

(e.g., `static_patch`, `learned_trigger`)

-   **Activation Clustering**
    
-   **Spectral Signatures**
    
-   **Anomaly Detection** (LOF, Isolation Forest)
    
-   **Pruning**
    
-   **Fine-Pruning**
    
-   **Model Inspection**
    

----------

### 5.3 Evasion Attacks

(e.g., `FGSM`, `PGD`, `DeepFool`, `SPSA`, `NES`)

-   **Adversarial Training**
    
-   **Gradient Masking**
    
-   **Randomized Smoothing**
    
-   **JPEG Preprocessing**
    

----------


All defenses are configurable through a unified YAML profile and integrated with upstream modules for threat configuration and risk analysis. The pipeline supports automatic model loading, retraining, mitigation, and standardized evaluation through JSON and Markdown reports.

Results are tailored to each attack type, including:
- **For Backdoor attacks:** Clean accuracy, Attack Success Rate (ASR) after defense, and per-class breakdowns.
- **For Evasion attacks:** Clean accuracy, adversarial accuracy, and per-class breakdowns.
- **For Data Poisoning attacks:** Clean accuracy, per-class breakdowns, and specific metrics related to sample removal or anomaly detection (e.g., number of removed samples, identified anomalous classes).
Metadata about the defense's behavior and decisions is also included.

----------

### 5.4 Future Work

With defenses now covering data poisoning, backdoor, and evasion threats, upcoming extensions to Module 4 may include:

-   **Defense Orchestration**:  
    Combining multiple defenses adaptively, based on threat intensity and model behavior.
    
-   **Auto-tuning of Defense Parameters**:  
    Automatically adjusting thresholds and hyperparameters using search or validation-driven strategies.
    
-   **Visualization Dashboards** (Modules 5–6):  
    Providing visual, interactive comparisons across defenses and attack types to guide decision-making and deployment readiness.
    

----------

This modular and extensible design positions **Safe-DL** as a practical and research-ready framework for defending deep learning pipelines in adversarial settings.



